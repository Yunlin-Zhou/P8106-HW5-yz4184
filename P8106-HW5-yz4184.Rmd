---
title: "P8106 HW5 yz4184"
author: "Yunlin Zhou"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, results='hide'}
library(tidyverse)
library(caret)
library(e1071)
library(kernlab)
library(ISLR)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
```

# Problem 1

```{r, results='hide'}
# import data
dat = read.csv("./auto.csv")%>%
  na.omit() %>% 
  mutate(
    cylinders = as.factor(cylinders),
         year = as.factor(year),
         origin = as.factor(origin),
    mpg_cat = factor(mpg_cat, levels = c("low", "high")))

# divide data into two parts (training and test)
set.seed(1)
rowTrain <- createDataPartition(y = dat$mpg_cat,
                                p = 0.7,
                                list = FALSE)

train_df = dat[rowTrain,]
test_df = dat[-rowTrain,]
```

## Part a

### Fit a support vector classifier (linear kernel) to the training data.

```{r}
set.seed(1)
linear.tune <- tune.svm( mpg_cat ~ . ,
data = train_df,
kernel = "linear",
cost = exp(seq(-1,3,len=50)),
scale = TRUE)
plot(linear.tune)

best.linear <- linear.tune$best.model
summary(best.linear)
```

According to the cost-error plot and best model summary above, we can conclude that the best tuning parameter c is 3.072369.

There are 50 support vectors in the optimal support vector classifier with a linear kernel.

### Training error rate

```{r}
#train error
pred.linear.train <- predict(best.linear, newdata = train_df)
confusionMatrix(data = pred.linear.train, 
                reference = train_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.9601, so the training error rate is (1-0.9601)*100% = 3.99% .

### Test error rate

```{r}
#test error
pred.linear.test <- predict(best.linear, newdata = test_df)
confusionMatrix(data = pred.linear.test, 
                reference = test_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.8966, so the test error rate is (1-0.8966)*100% = 10.34% .

\newpage

## Part b

### Fit a support vector machine with a radial kernel to the training data.

```{r}
set.seed(1)
radial.tune <- tune.svm( mpg_cat ~ . ,
                         data = train_df,
                        kernel = "radial", 
                        cost = exp(seq(-1,4,len=20)),
                        gamma = exp(seq(-6,-2,len=20)))

plot(radial.tune, transform.y = log, transform.x = log, 
     color.palette = terrain.colors)

radial.tune$best.parameters

best.radial <- radial.tune$best.model
summary(best.radial)
```

According to the gamma-cost plot and best parameters summary above, we can conclude that the best tuning parameters, gamma and cost, of the support vector machine are 0.07196474	 and 32.25536.

There are 54 support vectors in the optimal support vector classifier with a linear kernel.

### Training error rate

```{r}
#train error
pred.radial <- predict(best.radial, newdata = train_df)

confusionMatrix(data = pred.radial, 
                reference = train_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.9891, so the training error rate is (1-0.9891)*100% = 1.09% .

### Test error rate

```{r}
#test error
pred.radial <- predict(best.radial, newdata = test_df)

confusionMatrix(data = pred.radial, 
                reference = test_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.8879, so the test error rate is (1-0.8879)*100% = 11.21% .

\newpage

# Problem 2

```{r, results='hide'}
# import data
data(USArrests)
arrests_df = USArrests %>%
  as.data.frame() %>%
  janitor::clean_names()
```

## Part a

### Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
hc.complete <- hclust(dist(arrests_df), method = "complete")
```

### Cut the dendrogram at a height that results in three distinct clusters.

```{r}
fviz_dend(hc.complete, k = 3,
cex = 0.3,
palette = "jco",
color_labels_by_k = TRUE,
rect = TRUE, rect_fill = TRUE, rect_border = "jco",
labels_track_height = 2.5)
```


### States belong to the first cluster

```{r}
state_clusters = cutree(hc.complete, 3)

row.names(arrests_df[state_clusters == 1,])
```

### States belong to the second cluster

```{r}
row.names(arrests_df[state_clusters == 2,])
```

### States belong to the third cluster

```{r}
row.names(arrests_df[state_clusters == 3,])
```

\newpage

## Part b

### Scaling the variables to have standard deviation one.

```{r}
arrests_df_scale = scale(arrests_df)
```

###  Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
hc.complete.2 <- hclust(dist(arrests_df_scale), method = "complete")
```

### Cut the dendrogram at a height that results in three distinct clusters.

```{r}
fviz_dend(hc.complete.2, k = 3,
cex = 0.3,
palette = "jco",
color_labels_by_k = TRUE,
rect = TRUE, rect_fill = TRUE, rect_border = "jco",
labels_track_height = 2.5)
```


### States belong to the first cluster

```{r}
state_clusters_2 = cutree(hc.complete.2, 3)

row.names(arrests_df[state_clusters_2 == 1,])
```

### States belong to the second cluster

```{r}
row.names(arrests_df[state_clusters_2 == 2,])
```

### States belong to the third cluster

```{r}
row.names(arrests_df[state_clusters_2 == 3,])
```

\newpage

## Part c

Scaling the variables changed the clustering results.

Since many clustering algorithms require some definition of distance, if you do not scale and center your data, you may give attributes which have larger magnitudes more importance.

If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature.

In this problem, we are using Euclidean distance. In this data set, the variable urban_pop has an incomparable units to other variables.

In my opinion, the variables should be scaled before the inter-observation dissimilarities are computed. So that our variables are in comparable units and the algorithm could assign equal weight to the variables.
