---
title: "P8106 HW5 yz4184"
author: "Yunlin Zhou"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, results='hide'}
library(tidyverse)
library(caret)
library(e1071)
library(kernlab)
library(ISLR)
```

# Problem 1

```{r, results='hide'}
# import data
dat = read.csv("./auto.csv")%>%
  na.omit() %>% 
  mutate(
    cylinders = as.factor(cylinders),
         year = as.factor(year),
         origin = as.factor(origin),
    mpg_cat = factor(mpg_cat, levels = c("low", "high")))

# divide data into two parts (training and test)
set.seed(1)
rowTrain <- createDataPartition(y = dat$mpg_cat,
                                p = 0.7,
                                list = FALSE)

train_df = dat[rowTrain,]
test_df = dat[-rowTrain,]
```

## Part a

### Fit a support vector classifier (linear kernel) to the training data.

```{r}
set.seed(1)
linear.tune <- tune.svm( mpg_cat ~ . ,
data = train_df,
kernel = "linear",
cost = exp(seq(-1,3,len=50)),
scale = TRUE)
plot(linear.tune)

best.linear <- linear.tune$best.model
summary(best.linear)
```

According to the cost-error plot and best model summary above, we can conclude that the best tuning parameter c is 3.072369.

There are 50 support vectors in the optimal support vector classifier with a linear kernel.

### Training error rate

```{r}
#train error
pred.linear.train <- predict(best.linear, newdata = train_df)
confusionMatrix(data = pred.linear.train, 
                reference = train_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.9601, so the training error rate is (1-0.9601)*100% = 3.99% .

### Test error rate

```{r}
#test error
pred.linear.test <- predict(best.linear, newdata = test_df)
confusionMatrix(data = pred.linear.test, 
                reference = test_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.8966, so the test error rate is (1-0.8966)*100% = 10.34% .

\newpage

## Part b

### Fit a support vector machine with a radial kernel to the training data.

```{r}
set.seed(1)
radial.tune <- tune.svm( mpg_cat ~ . ,
                         data = train_df,
                        kernel = "radial", 
                        cost = exp(seq(-1,4,len=20)),
                        gamma = exp(seq(-6,-2,len=20)))

plot(radial.tune, transform.y = log, transform.x = log, 
     color.palette = terrain.colors)

radial.tune$best.parameters

best.radial <- radial.tune$best.model
summary(best.radial)
```

According to the gamma-cost plot and best parameters summary above, we can conclude that the best tuning parameters, gamma and cost, of the support vector machine are 0.07196474	 and 32.25536.

There are 54 support vectors in the optimal support vector classifier with a linear kernel.

### Training error rate

```{r}
#train error
pred.radial <- predict(best.radial, newdata = train_df)

confusionMatrix(data = pred.radial, 
                reference = train_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.9891, so the training error rate is (1-0.9891)*100% = 1.09% .

### Test error rate

```{r}
#test error
pred.radial <- predict(best.radial, newdata = test_df)

confusionMatrix(data = pred.radial, 
                reference = test_df$mpg_cat)
```

According to the confusion Matrix above, the accuracy is 0.8879, so the test error rate is (1-0.8879)*100% = 11.21% .

\newpage

# Problem 2

```{r}
# import data
data(USArrests)
arrests_df = USArrests %>%
  as.data.frame() %>%
  janitor::clean_names()
```

